#  Sign Language Recognition with ResNet50 + LSTM on LSA64 Dataset

This project builds a **deep learning pipeline** to recognize **Argentinian Sign Language (LSA)** signs from video, using:
- Transfer learning (**ResNet50**) to extract spatial features from frames
- Temporal modeling (**LSTM**) to learn motion & sequence patterns
- PyTorch for the complete implementation

> This repository shows the real end-to-end workflow: data loading, preprocessing, model architecture, training, and evaluation.


## **Project Goal**
Recognize sign language videos from the **LSA64 dataset** by:
- Extracting key frames from each video
- Using ResNet50 (pre-trained on ImageNet) to get per-frame features
- Feeding frame features into an LSTM network to learn temporal dynamics
- Training & evaluating the model on real data

Used the **LSA64 dataset** (64 signs, 10 subjects, 3,200 videos):  
ðŸ”— [LSA64 dataset link](https://facundoq.github.io/datasets/lsa64/)

## **Project Structure**
```plaintext
sign-language-recognition-lsa64/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ SLD.ipynb                # Main notebook with actual code & experiments
â”œâ”€â”€ src/                         # ðŸ› Reusable modules matching notebook logic
â”‚   â”œâ”€â”€ dataset.py               # PyTorch Dataset: video loading & frame extraction
â”‚   â”œâ”€â”€ model.py                 # ResNet50 + LSTM model
â”‚   â”œâ”€â”€ train.py                 # Training & evaluation loops
â”‚   â””â”€â”€ utils.py                 # Plotting etc.
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore



